{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee2ffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-compat\n",
      "  Downloading pandas_compat-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\roland\\anaconda3\\lib\\site-packages (from pandas-compat) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\roland\\anaconda3\\lib\\site-packages (from pandas->pandas-compat) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\roland\\anaconda3\\lib\\site-packages (from pandas->pandas-compat) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\roland\\anaconda3\\lib\\site-packages (from pandas->pandas-compat) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\roland\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->pandas-compat) (1.16.0)\n",
      "Installing collected packages: pandas-compat\n",
      "Successfully installed pandas-compat-0.1.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install pandas-compat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1053854",
   "metadata": {},
   "source": [
    "## Acquisition model/lookalike - logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3692b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"customer_acquisition.csv\")\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop(\"conversion\", axis=1)\n",
    "y = data[\"conversion\"]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall: {rec:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea500d",
   "metadata": {},
   "source": [
    "## Churn model - Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06452400",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16112\\1397162427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Load the Online Retail dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/online_retail.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Pre-processing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;31m# open URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    668\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Content-Encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the Online Retail dataset\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/online_retail.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Pre-processing\n",
    "df = df[df['Quantity'] > 0] # Keep only positive quantities\n",
    "df['Amount'] = df['Quantity'] * df['UnitPrice']\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate']) # Convert to datetime format\n",
    "df = df.drop(['InvoiceNo', 'Description', 'Quantity', 'UnitPrice'], axis=1)\n",
    "df = df.groupby(['CustomerID', 'InvoiceDate']).sum().reset_index() # Group by CustomerID and InvoiceDate and sum amounts\n",
    "df = df.sort_values(by=['CustomerID', 'InvoiceDate'], ascending=[True, True]) # Sort by CustomerID and InvoiceDate\n",
    "df['Acquisition'] = df.groupby('CustomerID').InvoiceDate.apply(lambda x: (x - x.shift(1)).dt.days > 30).astype(int) # Calculate acquisition as True if time difference between invoices is greater than 30 days\n",
    "df = df.dropna() # Drop missing values\n",
    "df = df[['Amount', 'Acquisition']] # Keep only relevant columns\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop('Acquisition', axis=1)\n",
    "y = df['Acquisition']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of different models to be looped through\n",
    "models = [LogisticRegression(), DecisionTreeClassifier(), KNeighborsClassifier(), RandomForestClassifier(), SVC()]\n",
    "\n",
    "# Dictionary to store the results of each model\n",
    "results = {}\n",
    "\n",
    "# Loop through the models\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results[type(model).__name__] = [accuracy, precision, recall, f1]\n",
    "\n",
    "# Find the model with the highest F1 score\n",
    "best_model = max(results, key=lambda x: results[x][3])\n",
    "\n",
    "# Print the results of each model\n",
    "print(\"Results for each model:\")\n",
    "for model in results:\n",
    "    print(f\"{model}: Accuracy = {results[model][0]:.3f}, Precision = {results[model][1]:.3f}, Recall = {results[model][2]:.3f}, F1 Score = {results[model][3]:.3f}\")\n",
    "\n",
    "# Print the best model based on F1 score\n",
    "print(f\"\\nBest model is {best_model} with F1 Score = {results[best_model][3]:.3f}\")\n",
    "\n",
    "# Calculate the confusion matrix for the best model\n",
    "best_model = globals()[best_model]()\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix for {best_model.__class__.__name__}:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ce748f",
   "metadata": {},
   "source": [
    "## Churn - Regression looping through - nuance vs. number of variables vs imbalanced (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d115532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the customer data into a pandas DataFrame\n",
    "customers = pd.read_csv(\"customer_data.csv\")\n",
    "\n",
    "# Split the data into features (X) and target (y) variables\n",
    "X = customers.drop(\"churn\", axis=1)\n",
    "y = customers[\"churn\"]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# List of models to loop through\n",
    "models = [LogisticRegression(solver='lbfgs'), \n",
    "          DecisionTreeClassifier(), \n",
    "          RandomForestClassifier(), \n",
    "          GradientBoostingClassifier(), \n",
    "          SVC(probability=True), \n",
    "          KNeighborsClassifier()]\n",
    "\n",
    "# Dictionary to store the ROC AUC scores for each model\n",
    "scores = {}\n",
    "\n",
    "# Loop through each model and fit it to the training data\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the probabilities of churn for the test data\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate the performance of the model using the ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_probs)\n",
    "    scores[type(model).__name__] = roc_auc\n",
    "    \n",
    "# Find the model with the highest ROC AUC score\n",
    "best_model_name = max(scores, key=scores.get)\n",
    "best_model = [model for model in models if type(model).__name__ == best_model_name][0]\n",
    "\n",
    "print(\"Best Model:\", best_model_name)\n",
    "print(\"Best Model ROC AUC:\", scores[best_model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d698570",
   "metadata": {},
   "source": [
    "## Cross sell - Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"cross_selling.csv\")\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop(\"cross_sold\", axis=1)\n",
    "y = data[\"cross_sold\"]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the decision tree model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall: {rec:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca1f9d",
   "metadata": {},
   "source": [
    "## Customer up-sell - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6bcf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"up_sell.csv\")\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop(\"up_sold\", axis=1)\n",
    "y = data[\"up_sold\"]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "print(f\"Precision: {prec:.3f}\")\n",
    "print(f\"Recall: {rec:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbecdf1",
   "metadata": {},
   "source": [
    "## Recommendation Engine - Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"retail_dataset.csv\")\n",
    "\n",
    "# Divide the data into features (X) and target (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc02b4b",
   "metadata": {},
   "source": [
    "## Recommendation engine - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"retail_dataset.csv\")\n",
    "\n",
    "# Divide the data into features (X) and target (y)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the SVM model\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b999b",
   "metadata": {},
   "source": [
    "## Segmentation agglomerative, kmeans, & DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the customer data\n",
    "customers = pd.read_csv(\"customer_data.csv\")\n",
    "\n",
    "# Normalize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "customers_scaled = scaler.fit_transform(customers)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test = train_test_split(customers_scaled, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize a list to store the models and their performance scores\n",
    "models = []\n",
    "\n",
    "# Loop through multiple models\n",
    "for model_type in [KMeans, AgglomerativeClustering, DBSCAN]:\n",
    "    for n_clusters in range(2, 10):\n",
    "        # Fit the model to the training data\n",
    "        model = model_type(n_clusters=n_clusters)\n",
    "        model.fit(X_train)\n",
    "        \n",
    "        # Predict the clusters for the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate the silhouette score to evaluate the model's performance\n",
    "        score = silhouette_score(X_test, y_pred)\n",
    "        \n",
    "        # Store the model and its performance score in the list\n",
    "        models.append((model_type, n_clusters, score))\n",
    "\n",
    "# Sort the models based on their performance scores\n",
    "models = sorted(models, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Select the best model\n",
    "best_model = models[0]\n",
    "\n",
    "# Fit the best model to the entire dataset\n",
    "model = best_model[0](n_clusters=best_model[1])\n",
    "model.fit(customers_scaled)\n",
    "\n",
    "# Predict the clusters for each customer\n",
    "labels = model.predict(customers_scaled)\n",
    "\n",
    "# Add the cluster labels to the customer data as a new column\n",
    "customers[\"Cluster\"] = labels\n",
    "\n",
    "# Group the customer data by cluster to get a summary of each cluster\n",
    "cluster_summary = customers.groupby(\"Cluster\").mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62956cd",
   "metadata": {},
   "source": [
    "## Engagement model - Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdcf8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the log-level data into a pandas dataframe\n",
    "df = pd.read_csv(\"log_level_data.csv\")\n",
    "\n",
    "# Prepare the data for modeling\n",
    "X = df.drop([\"customer_id\", \"engagement_score\"], axis=1)\n",
    "y = df[\"engagement_score\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the Random Forest Regressor model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the engagement scores for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error between the predicted and actual engagement scores\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: \", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ed48a",
   "metadata": {},
   "source": [
    "## Media mix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(\"online_retail_dataset.csv\")\n",
    "\n",
    "# preprocess the data\n",
    "df = df.dropna()\n",
    "df['spend_amount'] = df['Quantity'] * df['UnitPrice']\n",
    "df = df.groupby(['InvoiceDate', 'Channel'], as_index=False).agg({'spend_amount': 'sum'})\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "df = df.set_index('InvoiceDate')\n",
    "\n",
    "# create pivot table for channels and time period\n",
    "pivot_table = df.pivot_table(values='spend_amount', index=df.index, columns='Channel')\n",
    "\n",
    "# create channel spend by month\n",
    "pivot_table = pivot_table.resample('M').sum()\n",
    "pivot_table = pivot_table.fillna(0)\n",
    "\n",
    "# create channel spend as a percentage of total spend\n",
    "total_spend = pivot_table.sum(axis=1)\n",
    "pivot_table = pivot_table.divide(total_spend, axis=0)\n",
    "\n",
    "# create media mix model\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(pivot_table)\n",
    "components = pca.transform(pivot_table)\n",
    "pivot_table_pca = pd.DataFrame(data=components, columns=['Component 1', 'Component 2'], index=pivot_table.index)\n",
    "\n",
    "# plot the media mix model\n",
    "sns.scatterplot(x='Component 1', y='Component 2', data=pivot_table_pca)\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.title('Media Mix Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36602d96",
   "metadata": {},
   "source": [
    "## Marketing mix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcf8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load in your data\n",
    "df = pd.read_csv(\"marketing_data.csv\")\n",
    "\n",
    "# Define the target variable and features\n",
    "target = \"sales\"\n",
    "features = [\"product\", \"price\", \"promotion\", \"place\"]\n",
    "\n",
    "# Create a linear regression model\n",
    "model = smf.ols(f\"{target} ~ {' + '.join(features)}\", data=df)\n",
    "\n",
    "# Fit the model to the data\n",
    "results = model.fit()\n",
    "\n",
    "# Print out the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a203bc",
   "metadata": {},
   "source": [
    "## Attribution model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8063c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sample data\n",
    "data = pd.read_csv(\"attribution_data.csv\")\n",
    "\n",
    "# Calculate the time decay factor for each touchpoint\n",
    "data['Time'] = (data['Conversion_Time'] - data['Touchpoint_Time']).dt.total_seconds() / (24 * 60 * 60)\n",
    "data['Time_Decay'] = 1 / (1 + data['Time'])\n",
    "\n",
    "# Group the data by customer ID\n",
    "grouped_data = data.groupby(\"Customer_ID\")\n",
    "\n",
    "# Calculate the weighted conversion value for each touchpoint\n",
    "grouped_data['Weighted_Conversion_Value'] = grouped_data['Conversion_Value'] * grouped_data['Time_Decay']\n",
    "\n",
    "# Sum the weighted conversion value for each channel\n",
    "attributed_value = grouped_data.groupby(\"Channel\")['Weighted_Conversion_Value'].sum().reset_index()\n",
    "\n",
    "# Normalize the attributed value to 100%\n",
    "attributed_value['Attributed_Value_Percentage'] = attributed_value['Weighted_Conversion_Value'] / attributed_value['Weighted_Conversion_Value'].sum() * 100\n",
    "\n",
    "# Print the results\n",
    "print(attributed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0cc23c",
   "metadata": {},
   "source": [
    "## Topic modeling - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Create the document-term matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "# Run the LDA algorithm\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Print the top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "print_top_words(lda, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b6a7e",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a88ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Create the document-term matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(df[\"text\"])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_term_matrix, df[\"sentiment\"], test_size=0.2, random_state=0)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the sentiment of the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bd966",
   "metadata": {},
   "source": [
    "## Customer lifetime value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e9fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lifetimes import ParetoNBDFitter\n",
    "\n",
    "# Create a dummy customer dataset\n",
    "data = {'customer_id': [1, 2, 3, 4, 5],\n",
    "        'frequency': [2, 3, 1, 5, 4],\n",
    "        'recency': [30, 20, 10, 50, 40],\n",
    "        'T': [60, 50, 40, 80, 70],\n",
    "        'monetary_value': [100, 150, 50, 200, 175]}\n",
    "\n",
    "# Convert the data into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the Pareto/NBD model\n",
    "model = ParetoNBDFitter(penalizer_coef=0.0)\n",
    "\n",
    "# Fit the model to the customer data\n",
    "model.fit(df['frequency'], df['recency'], df['T'])\n",
    "\n",
    "# Predict the customer lifetime value for each customer\n",
    "clv = model.customer_lifetime_value(df['frequency'], df['recency'], df['T'], df['monetary_value'], discount_rate=0.01)\n",
    "\n",
    "# Add the predicted CLV to the customer data DataFrame\n",
    "df['CLV'] = clv\n",
    "\n",
    "# Print the customer data with the predicted CLV\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385dceb",
   "metadata": {},
   "source": [
    "## Loyalty management - linear, ridge, & LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fb9360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate a dummy dataset\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=0.1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train and evaluate multiple models\n",
    "models = [LinearRegression(), Ridge(), Lasso()]\n",
    "model_names = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    # Train the model\n",
    "    model.fit(train_data, train_labels)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(test_labels, predictions)\n",
    "    r2 = r2_score(test_labels, predictions)\n",
    "\n",
    "    print(\"Model: {}\".format(model_name))\n",
    "    print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
    "    print(\"R^2 Score: {:.2f}\".format(r2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79213340",
   "metadata": {},
   "source": [
    "## CSAT and NPS score (Based on range and score created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb40b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Calculate NPS scores\n",
    "df[\"nps\"] = df.apply(lambda row: (row[\"promoters\"] - row[\"detractors\"]) / row[\"total\"], axis=1) * 100\n",
    "\n",
    "# Convert NPS scores to range of -100 to 100\n",
    "df.loc[df[\"nps\"] > 100, \"nps\"] = 100\n",
    "df.loc[df[\"nps\"] < -100, \"nps\"] = -100\n",
    "\n",
    "# Calculate CSAT scores\n",
    "df[\"csat\"] = df[\"satisfaction\"] / df[\"total\"] * 100\n",
    "\n",
    "# Convert CSAT scores to range of -100 to 100\n",
    "df.loc[df[\"csat\"] > 100, \"csat\"] = 100\n",
    "df.loc[df[\"csat\"] < -100, \"csat\"] = -100\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df[[\"nps\", \"csat\", \"features_1\", \"features_2\"]].values\n",
    "y = df[\"nps\"].values\n",
    "\n",
    "# Define the list of models to loop through\n",
    "models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor()]\n",
    "\n",
    "# Loop through the models\n",
    "best_r2 = -np.inf\n",
    "best_model = None\n",
    "for model in models:\n",
    "    # Fit the model\n",
    "    reg = model.fit(X, y)\n",
    "    \n",
    "    # Predict the target\n",
    "    y_pred = reg.predict(X)\n",
    "    \n",
    "    # Calculate the R^2 score\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    # Update the best model if necessary\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model = model\n",
    "\n",
    "# Print the best model and its R^2 score\n",
    "print(\"Best model:\", best_model)\n",
    "print(\"R^2 score:\", best_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b298844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conjoint analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the data into a pandas dataframe\n",
    "df = pd.read_csv(\"conjoint_data.csv\")\n",
    "\n",
    "# Define the features and target variable\n",
    "features = [\"Attribute 1\", \"Attribute 2\", \"Attribute 3\", \"Attribute 4\"]\n",
    "target = \"Choice\"\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list to store the results of all models\n",
    "results = []\n",
    "\n",
    "# Loop through multiple models\n",
    "for i in range(1, 4):\n",
    "    # Initialize the model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Append the results of each model to the list\n",
    "    results.append([i, mse])\n",
    "\n",
    "# Find the model with the lowest mean squared error\n",
    "best_model = min(results, key=lambda x: x[1])[0]\n",
    "\n",
    "# Print the best model\n",
    "print(\"The best model is Model {} with a mean squared error of {}\".format(best_model, min(results, key=lambda x: x[1])[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lead scoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('path_to_dataset.csv')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target_variable', axis=1), df['target_variable'], test_size=0.3, random_state=42)\n",
    "\n",
    "# Define a list of models to loop through\n",
    "models = [LogisticRegression(random_state=42), RandomForestClassifier(n_estimators=100, random_state=42), GradientBoostingClassifier(random_state=42)]\n",
    "\n",
    "# Loop through the models and fit each on the training data\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    print(model, \"AUC:\", auc)\n",
    "\n",
    "# Choose the best performing model\n",
    "best_model = models[np.argmax([roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) for model in models])]\n",
    "\n",
    "# Train the best model on the complete dataset\n",
    "best_model.fit(df.drop('target_variable', axis=1), df['target_variable'])\n",
    "\n",
    "# Make predictions for the complete dataset\n",
    "df['propensity_score'] = best_model.predict_proba(df.drop('target_variable', axis=1))[:, 1]\n",
    "\n",
    "# Apply deciles to the propensity score\n",
    "df['decile'] = pd.qcut(df['propensity_score'], 10, labels=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
